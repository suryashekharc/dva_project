# -*- coding: utf-8 -*-
"""huggingface.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pe7CmOPQAykq0K3T-xProE3iclw2Twv3
"""

!pip install detoxify
import pandas as pd
import numpy as np
import os
from detoxify import Detoxify

model = Detoxify('original')

files = os.listdir()
states=[]
mean_identity_attack = []
mean_insult=[]
mean_obscene=[]
mean_severe_toxicity=[]
mean_threat=[]
mean_toxicity=[]
files = os.listdir()
csv_files = [file for file in files if file.endswith('csv')]

for file in csv_files[22:]:
  print(file)
  states.append(file.split("_")[0])
  identity_attack = []
  insult=[]
  obscene=[]
  severe_toxicity=[]
  threat=[]
  toxicity=[]
  temp_df = pd.read_csv(file)
  comments = temp_df['body'].tolist()
  for comment in comments:
      result = model.predict(comment)
      identity_attack.append(result['identity_attack'])
      insult.append(result['insult'])
      obscene.append(result['obscene'])
      severe_toxicity.append(result['severe_toxicity'])
      threat.append(result['threat'])
      toxicity.append(result['toxicity'])
  mean_identity_attack.append(np.mean(identity_attack))
  mean_insult.append( np.mean(insult))
  mean_obscene.append(np.mean(obscene))
  mean_severe_toxicity.append(np.mean(severe_toxicity))
  mean_threat.append(np.mean(threat))
  mean_toxicity.append(np.mean(toxicity))

res_df = pd.DataFrame(list(zip(states,mean_identity_attack, mean_insult,mean_obscene,mean_severe_toxicity,mean_threat,mean_toxicity)),
                      columns =['Location', 'identity_attack', 'insult','obscene','severe_toxicity','threat','toxicity'])

res_df.to_csv("output.csv")